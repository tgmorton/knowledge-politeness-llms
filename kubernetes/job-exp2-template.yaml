---
# Kubernetes Job Template for Experiment 2 (Direct Model Scoring)
#
# Experiment 2 uses direct model access for probability extraction:
# - Study 1 Exp 2: Probability distributions over state queries + knowledge question
# - Study 2 Exp 2: Probability distributions over politeness judgments
#
# Requirements:
# - NO vLLM server needed (loads model directly in Job)
# - GPU required (to load and score with model)
# - HuggingFace model will be downloaded on first run
#
# Usage:
#   1. NO need for vLLM deployment
#   2. Update MODEL_PATH, STUDY variables below
#   3. Apply: kubectl apply -f job-exp2-template.yaml
#
# NOTE: This job will take longer than Exp1 due to model loading time (~5-10 min)

apiVersion: batch/v1
kind: Job
metadata:
  name: grace-study1-exp2-gemma2b
  namespace: lemn-lab
  labels:
    app: grace-experiment
    study: "1"
    experiment: "2"
    model: gemma-2b
spec:
  # Don't retry on failure - let user debug
  backoffLimit: 0

  # Clean up completed jobs after 1 hour
  ttlSecondsAfterFinished: 3600

  template:
    metadata:
      labels:
        app: grace-experiment
        study: "1"
        experiment: "2"
        model: gemma-2b
    spec:
      restartPolicy: Never

      # Node selection for GPU availability
      nodeSelector:
        nvidia.com/gpu.product: NVIDIA-A100-SXM4-80GB

      # Tolerations for GPU nodes
      tolerations:
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule

      containers:
      - name: experiment-runner
        image: gitlab-registry.nrp-nautilus.io/thmorton/grace-project/query-generator:latest
        imagePullPolicy: Always  # Always pull latest from registry

        command:
          - python3
          - /app/src/query_study1_exp2.py

        args:
          - --input=/data/input/study1.csv
          - --output=/data/results/study1_exp2_gemma2b_$(TIMESTAMP).csv
          - --model-path=google/gemma-2-2b-it
          - --model-name=gemma-2-2b-it

        env:
          - name: TIMESTAMP
            value: "20250119_000000"  # Set at Job creation time
          - name: PYTHONUNBUFFERED
            value: "1"  # Immediate log output
          - name: HF_HOME
            value: /tmp/huggingface
          - name: TRANSFORMERS_CACHE
            value: /tmp/huggingface

        # GPU resources needed for direct model scoring
        # Gemma-2B fits comfortably on 1x A100-80GB
        resources:
          requests:
            nvidia.com/gpu: 1
            memory: "32Gi"
            cpu: "8"
          limits:
            nvidia.com/gpu: 1
            memory: "38Gi"    # 119% of request - COMPLIANT
            cpu: "9600m"      # 120% of request - COMPLIANT

        volumeMounts:
          - name: input-data
            mountPath: /data/input
            readOnly: true

          - name: results
            mountPath: /data/results

          - name: shm
            mountPath: /dev/shm

      volumes:
        - name: input-data
          persistentVolumeClaim:
            claimName: grace-input-data

        - name: results
          persistentVolumeClaim:
            claimName: grace-results

        # Shared memory for PyTorch
        - name: shm
          emptyDir:
            medium: Memory
            sizeLimit: 16Gi

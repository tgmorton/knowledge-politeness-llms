---
# Cache warmup job for Llama 3.1 70B
# Pre-fetches model files to shared storage
# Model size: ~140GB (unquantized), ~35GB (AWQ quantized)

apiVersion: batch/v1
kind: Job
metadata:
  name: cache-warmup-llama-70b
  namespace: lemn-lab
  labels:
    app: cache-warmup
    project: grace-experiments
    model: llama-70b
spec:
  ttlSecondsAfterFinished: 3600
  backoffLimit: 2
  template:
    metadata:
      labels:
        app: cache-warmup
        model: llama-70b
    spec:
      restartPolicy: OnFailure

      containers:
      - name: fetcher
        image: vllm/vllm-openai:latest

        command:
          - python3
          - -c
          - |
            from huggingface_hub import snapshot_download
            import os

            print("Starting download of meta-llama/Llama-3.1-70B-Instruct...")
            print("⚠️  This is a large model (~140GB), download may take 30-60 minutes")

            snapshot_download(
                repo_id="meta-llama/Llama-3.1-70B-Instruct",
                cache_dir="/models/.cache",
                local_dir_use_symlinks=False
            )

            print("✅ Download complete!")

            # Show size
            os.system("du -sh /models/.cache/models--meta-llama--Llama-3.1-70B-Instruct 2>/dev/null || echo 'Cache populated'")

        env:
          - name: HF_HOME
            value: /models/.cache
          - name: HF_HUB_CACHE
            value: /models/.cache
          - name: HF_TOKEN
            valueFrom:
              secretKeyRef:
                name: hf-token-thomas
                key: HF_TOKEN

        volumeMounts:
          - name: cache
            mountPath: /models

        resources:
          requests:
            memory: 8Gi
            cpu: "2"
          limits:
            memory: 9600Mi  # 120% of 8Gi (compliant)
            cpu: "2400m"    # 120% (compliant)

      volumes:
        - name: cache
          persistentVolumeClaim:
            claimName: thomas-grace-llama-70b-cache

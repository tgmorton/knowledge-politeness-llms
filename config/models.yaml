# Grace Project - Model Configurations
# RTX 3090 GPU configurations (24GB VRAM per card, 48 nodes available)
#
# All configurations are NRP-compliant:
# - Memory limits within 120% of requests
# - CPU limits within 120% of requests
# - Llama-70B requires 4-GPU exception (will request from NRP)

models:
  gemma-2b-rtx3090:
    display_name: "Gemma 2B"
    huggingface_name: google/gemma-2-2b-it
    description: "Smallest model, fast inference, good baseline"

    gpu:
      count: 1
      type: RTX-3090
      vram_per_gpu: 24GB
      node_selector: NVIDIA-GeForce-RTX-3090

    vllm:
      dtype: bfloat16
      quantization: null  # No quantization needed (~5GB model)
      max_model_len: 4096
      tensor_parallel_size: 1

    resources:
      memory_request: 16Gi
      memory_limit: 19Gi      # 118.75% (compliant ✅)
      cpu_request: "8"
      cpu_limit: "9600m"      # 120% (compliant ✅)
      shm_size: 8Gi

    deployment:
      name: vllm-gemma-2b-rtx3090
      service_name: vllm-gemma-2b

    replication:
      parallelism: 3  # 1 GPU per replication, can run 3 in parallel

  llama-3b-rtx3090:
    display_name: "Llama 3.2 3B"
    huggingface_name: meta-llama/Llama-3.2-3B-Instruct
    description: "Small Llama model, matched comparison with Gemma-2B"

    gpu:
      count: 1
      type: RTX-3090
      vram_per_gpu: 24GB
      node_selector: NVIDIA-GeForce-RTX-3090

    vllm:
      dtype: bfloat16
      quantization: null  # No quantization needed (~6GB model)
      max_model_len: 4096
      tensor_parallel_size: 1

    resources:
      memory_request: 16Gi
      memory_limit: 19Gi      # 118.75% (compliant ✅)
      cpu_request: "8"
      cpu_limit: "9600m"      # 120% (compliant ✅)
      shm_size: 8Gi

    deployment:
      name: vllm-llama-3b-rtx3090
      service_name: vllm-llama-3b

    replication:
      parallelism: 3  # 1 GPU per replication, can run 3 in parallel

  gemma-9b-rtx3090:
    display_name: "Gemma 9B"
    huggingface_name: google/gemma-2-9b-it
    description: "Mid-size model, good quality/speed balance"

    gpu:
      count: 1
      type: RTX-3090
      vram_per_gpu: 24GB
      node_selector: NVIDIA-GeForce-RTX-3090

    vllm:
      dtype: bfloat16
      quantization: null  # Fits without quantization (~18GB model)
      max_model_len: 4096
      tensor_parallel_size: 1

    resources:
      memory_request: 20Gi
      memory_limit: 24Gi      # 120% (compliant ✅)
      cpu_request: "12"
      cpu_limit: "14400m"     # 120% (compliant ✅)
      shm_size: 10Gi

    deployment:
      name: vllm-gemma-9b-rtx3090
      service_name: vllm-gemma-9b

    replication:
      parallelism: 3  # 1 GPU per replication, can run 3 in parallel

  llama-8b-rtx3090:
    display_name: "Llama 3.1 8B"
    huggingface_name: meta-llama/Llama-3.1-8B-Instruct
    description: "Medium Llama model, matched comparison with Gemma-9B"

    gpu:
      count: 1
      type: RTX-3090
      vram_per_gpu: 24GB
      node_selector: NVIDIA-GeForce-RTX-3090

    vllm:
      dtype: bfloat16
      quantization: null  # Fits without quantization (~16GB model)
      max_model_len: 4096
      tensor_parallel_size: 1

    resources:
      memory_request: 20Gi
      memory_limit: 24Gi      # 120% (compliant ✅)
      cpu_request: "12"
      cpu_limit: "14400m"     # 120% (compliant ✅)
      shm_size: 10Gi

    deployment:
      name: vllm-llama-8b-rtx3090
      service_name: vllm-llama-8b

    replication:
      parallelism: 3  # 1 GPU per replication, can run 3 in parallel

  gemma-27b-rtx3090:
    display_name: "Gemma 27B"
    huggingface_name: google/gemma-2-27b-it
    description: "Large Gemma model on 4x RTX 3090 GPUs (96GB total VRAM)"

    gpu:
      count: 4  # Tensor parallelism across 4 GPUs
      type: RTX-3090
      vram_per_gpu: 24GB
      node_selector: NVIDIA-GeForce-RTX-3090

    vllm:
      dtype: bfloat16
      quantization: null  # Full bfloat16, no quantization needed (96GB available)
      max_model_len: 4096
      tensor_parallel_size: 4

    resources:
      memory_request: 60Gi
      memory_limit: 72Gi      # 120% (compliant ✅)
      cpu_request: "16"
      cpu_limit: "19200m"     # 120% (compliant ✅)
      shm_size: 20Gi

    deployment:
      name: vllm-gemma-27b-rtx3090
      service_name: vllm-gemma-27b

    replication:
      parallelism: 1  # 4 GPUs per replication, run sequentially to avoid deadlocks

  llama-70b-rtx3090:
    display_name: "Llama 3.1 70B"
    huggingface_name: meta-llama/Llama-3.1-70B-Instruct
    description: "Largest model, requires 8 GPUs without quantization"

    gpu:
      count: 8  # 8x RTX-3090 for full bfloat16 precision
      type: RTX-3090
      vram_per_gpu: 24GB
      node_selector: NVIDIA-GeForce-RTX-3090

    vllm:
      dtype: bfloat16
      quantization: null  # No quantization, using 8 GPUs for full precision
      max_model_len: 4096
      tensor_parallel_size: 8

    resources:
      memory_request: 120Gi
      memory_limit: 144Gi     # 120% (compliant ✅)
      cpu_request: "48"
      cpu_limit: "57600m"     # 120% (compliant ✅)
      shm_size: 48Gi

    deployment:
      name: vllm-llama-70b-rtx3090
      service_name: vllm-llama-70b

    replication:
      parallelism: 1  # 8 GPUs per replication, MUST run sequentially

    notes: |
      This configuration uses 8 GPUs for full bfloat16 precision (no quantization).
      NRP automatically grants tolerations for 8-GPU jobs.

      Total VRAM: 8 × 24GB = 192GB (sufficient for ~140GB model + overhead)

  deepseek-r1-70b-rtx3090:
    display_name: "DeepSeek-R1 70B"
    huggingface_name: deepseek-ai/DeepSeek-R1
    description: "Reasoning model with explicit chain-of-thought, requires 8 GPUs without quantization"

    # Mark as reasoning model for special handling
    reasoning_model: true
    reasoning_tokens:
      start: "<think>"
      end: "</think>"

    gpu:
      count: 8  # 8x RTX-3090 for full bfloat16 precision
      type: RTX-3090
      vram_per_gpu: 24GB
      node_selector: NVIDIA-GeForce-RTX-3090

    vllm:
      dtype: bfloat16
      quantization: null  # No quantization, using 8 GPUs for full precision
      max_model_len: 4096
      tensor_parallel_size: 8

    resources:
      memory_request: 120Gi
      memory_limit: 144Gi     # 120% (compliant ✅)
      cpu_request: "48"
      cpu_limit: "57600m"     # 120% (compliant ✅)
      shm_size: 48Gi

    deployment:
      name: vllm-deepseek-r1-70b-rtx3090
      service_name: vllm-deepseek-r1-70b

    replication:
      parallelism: 1  # 8 GPUs per replication, MUST run sequentially

    notes: |
      Reasoning model with explicit chain-of-thought traces.

      Special handling required:
      - Captures reasoning in <think>...</think> tags
      - Extract reasoning traces separately from final answers
      - Token-level logprobs for both reasoning and answer tokens

      Uses 8 GPUs for full bfloat16 precision (no quantization).
      NRP automatically grants tolerations for 8-GPU jobs.

      Total VRAM: 8 × 24GB = 192GB (sufficient for ~140GB model + overhead)

# Common settings for all models
common:
  namespace: lemn-lab
  image_registry: gitlab-registry.nrp-nautilus.io/thmorton/grace-project

  secrets:
    hf_token: hf-token-thomas

  pvcs:
    results: thomas-grace-results
    model_cache: thomas-grace-model-cache

  tolerations:
    - key: nvidia.com/gpu
      operator: Exists
      effect: NoSchedule

  probes:
    liveness:
      path: /health
      port: 8000
      initial_delay_seconds: 600  # 10 minutes - safe for even 70B models
      period_seconds: 60           # Check every minute
      timeout_seconds: 10
      failure_threshold: 5         # Allow 5 failures (5 more minutes)

    readiness:
      path: /health
      port: 8000
      initial_delay_seconds: 300   # 5 minutes before routing traffic
      period_seconds: 30
      timeout_seconds: 10
      failure_threshold: 5

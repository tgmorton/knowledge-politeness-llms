# Grace Project - Model Configurations
# RTX 3090 GPU configurations (24GB VRAM per card, 48 nodes available)
#
# All configurations are NRP-compliant:
# - Memory limits within 120% of requests
# - CPU limits within 120% of requests
# - Llama-70B requires 4-GPU exception (will request from NRP)

models:
  gemma-2b-rtx3090:
    display_name: "Gemma 2B"
    huggingface_name: google/gemma-2-2b-it
    description: "Smallest model, fast inference, good baseline"

    gpu:
      count: 1
      type: RTX-3090
      vram_per_gpu: 24GB
      node_selector: NVIDIA-GeForce-RTX-3090

    vllm:
      dtype: bfloat16
      quantization: null  # No quantization needed (~5GB model)
      max_model_len: 4096
      tensor_parallel_size: 1

    resources:
      memory_request: 16Gi
      memory_limit: 19Gi      # 118.75% (compliant ✅)
      cpu_request: "8"
      cpu_limit: "9600m"      # 120% (compliant ✅)
      shm_size: 8Gi

    deployment:
      name: vllm-gemma-2b-rtx3090
      service_name: vllm-gemma-2b

  llama-3b-rtx3090:
    display_name: "Llama 3.2 3B"
    huggingface_name: meta-llama/Llama-3.2-3B-Instruct
    description: "Small Llama model, matched comparison with Gemma-2B"

    gpu:
      count: 1
      type: RTX-3090
      vram_per_gpu: 24GB
      node_selector: NVIDIA-GeForce-RTX-3090

    vllm:
      dtype: bfloat16
      quantization: null  # No quantization needed (~6GB model)
      max_model_len: 4096
      tensor_parallel_size: 1

    resources:
      memory_request: 16Gi
      memory_limit: 19Gi      # 118.75% (compliant ✅)
      cpu_request: "8"
      cpu_limit: "9600m"      # 120% (compliant ✅)
      shm_size: 8Gi

    deployment:
      name: vllm-llama-3b-rtx3090
      service_name: vllm-llama-3b

  gemma-9b-rtx3090:
    display_name: "Gemma 9B"
    huggingface_name: google/gemma-2-9b-it
    description: "Mid-size model, good quality/speed balance"

    gpu:
      count: 1
      type: RTX-3090
      vram_per_gpu: 24GB
      node_selector: NVIDIA-GeForce-RTX-3090

    vllm:
      dtype: bfloat16
      quantization: null  # Fits without quantization (~18GB model)
      max_model_len: 4096
      tensor_parallel_size: 1

    resources:
      memory_request: 20Gi
      memory_limit: 24Gi      # 120% (compliant ✅)
      cpu_request: "12"
      cpu_limit: "14400m"     # 120% (compliant ✅)
      shm_size: 10Gi

    deployment:
      name: vllm-gemma-9b-rtx3090
      service_name: vllm-gemma-9b

  llama-8b-rtx3090:
    display_name: "Llama 3.1 8B"
    huggingface_name: meta-llama/Llama-3.1-8B-Instruct
    description: "Medium Llama model, matched comparison with Gemma-9B"

    gpu:
      count: 1
      type: RTX-3090
      vram_per_gpu: 24GB
      node_selector: NVIDIA-GeForce-RTX-3090

    vllm:
      dtype: bfloat16
      quantization: null  # Fits without quantization (~16GB model)
      max_model_len: 4096
      tensor_parallel_size: 1

    resources:
      memory_request: 20Gi
      memory_limit: 24Gi      # 120% (compliant ✅)
      cpu_request: "12"
      cpu_limit: "14400m"     # 120% (compliant ✅)
      shm_size: 10Gi

    deployment:
      name: vllm-llama-8b-rtx3090
      service_name: vllm-llama-8b

  gemma-27b-rtx3090:
    display_name: "Gemma 27B"
    huggingface_name: google/gemma-2-27b-it
    description: "Large Gemma model on 2x L40 GPUs (88GB total VRAM)"

    gpu:
      count: 2  # Tensor parallelism across 2 GPUs
      type: L40
      vram_per_gpu: 44GB
      node_selector: NVIDIA-L40

    vllm:
      dtype: bfloat16
      quantization: null  # Full bfloat16, no quantization needed (88GB available)
      max_model_len: 4096
      tensor_parallel_size: 2

    resources:
      memory_request: 60Gi
      memory_limit: 72Gi      # 120% (compliant ✅)
      cpu_request: "16"
      cpu_limit: "19200m"     # 120% (compliant ✅)
      shm_size: 20Gi

    deployment:
      name: vllm-gemma-27b-rtx3090
      service_name: vllm-gemma-27b

  llama-70b-rtx3090:
    display_name: "Llama 3.1 70B"
    huggingface_name: meta-llama/Llama-3.1-70B-Instruct
    description: "Largest model, requires 4 GPUs with AWQ quantization"

    gpu:
      count: 4  # ⚠️ Requires NRP exception (default limit is 2 GPUs)
      type: RTX-3090
      vram_per_gpu: 24GB
      node_selector: NVIDIA-GeForce-RTX-3090

    vllm:
      dtype: bfloat16
      quantization: awq  # Required to fit in 4x24GB (~140GB model → ~35GB quantized)
      max_model_len: 4096
      tensor_parallel_size: 4

    resources:
      memory_request: 80Gi
      memory_limit: 96Gi      # 120% (compliant ✅)
      cpu_request: "32"
      cpu_limit: "38400m"     # 120% (compliant ✅)
      shm_size: 32Gi

    deployment:
      name: vllm-llama-70b-rtx3090
      service_name: vllm-llama-70b

    notes: |
      This configuration requires a 4-GPU exception from NRP.

      To request exception:
      1. Join NRP Matrix chat: https://matrix.to/#/#nrp:matrix.org
      2. Request 4-GPU pod allocation for Llama-70B inference
      3. Justification: Sequential deployment, <1 day per model, academic research

      Alternative: Skip this model on RTX 3090, run on A100 later

  deepseek-r1-70b-rtx3090:
    display_name: "DeepSeek-R1 70B"
    huggingface_name: deepseek-ai/DeepSeek-R1
    description: "Reasoning model with explicit chain-of-thought, requires 4 GPUs with AWQ"

    # Mark as reasoning model for special handling
    reasoning_model: true
    reasoning_tokens:
      start: "<think>"
      end: "</think>"

    gpu:
      count: 4  # ⚠️ Requires NRP exception (default limit is 2 GPUs)
      type: RTX-3090
      vram_per_gpu: 24GB
      node_selector: NVIDIA-GeForce-RTX-3090

    vllm:
      dtype: bfloat16
      quantization: awq  # Required to fit in 4x24GB (~140GB model → ~35GB quantized)
      max_model_len: 4096
      tensor_parallel_size: 4

    resources:
      memory_request: 80Gi
      memory_limit: 96Gi      # 120% (compliant ✅)
      cpu_request: "32"
      cpu_limit: "38400m"     # 120% (compliant ✅)
      shm_size: 32Gi

    deployment:
      name: vllm-deepseek-r1-70b-rtx3090
      service_name: vllm-deepseek-r1-70b

    notes: |
      Reasoning model with explicit chain-of-thought traces.

      Special handling required:
      - Captures reasoning in <think>...</think> tags
      - Extract reasoning traces separately from final answers
      - Token-level logprobs for both reasoning and answer tokens

      Same GPU requirements as Llama-70B (4-GPU exception needed).

# Common settings for all models
common:
  namespace: lemn-lab
  image_registry: gitlab-registry.nrp-nautilus.io/thmorton/grace-project

  secrets:
    hf_token: hf-token-thomas

  pvcs:
    results: thomas-grace-results
    model_cache: thomas-grace-model-cache

  tolerations:
    - key: nvidia.com/gpu
      operator: Exists
      effect: NoSchedule

  probes:
    liveness:
      path: /health
      port: 8000
      initial_delay_seconds: 600  # 10 minutes - safe for even 70B models
      period_seconds: 60           # Check every minute
      timeout_seconds: 10
      failure_threshold: 5         # Allow 5 failures (5 more minutes)

    readiness:
      path: /health
      port: 8000
      initial_delay_seconds: 300   # 5 minutes before routing traffic
      period_seconds: 30
      timeout_seconds: 10
      failure_threshold: 5

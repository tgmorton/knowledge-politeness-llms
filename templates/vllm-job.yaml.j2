# Jinja2 template for vLLM server as a Kubernetes Job
# Unlike Deployments, Jobs can request unlimited GPUs (no 2-GPU default limit)
#
# Template variables:
#   - model: Model configuration dict from models.yaml
#   - model_key: Model key (e.g., "gemma-2b-rtx3090")
#   - namespace: Kubernetes namespace
#   - pvcs: PVC names dict
#   - deployment: Deployment config (service_name, container_port, etc.)
#   - gpu: GPU config (count, node_selector)
#   - resources: Resource requirements
#   - vllm_args: vLLM server arguments

---
apiVersion: batch/v1
kind: Job
metadata:
  name: vllm-{{ model_key }}
  namespace: {{ namespace }}
  labels:
    app: vllm
    model: {{ model_key }}
    project: grace-experiments
    component: model-server
spec:
  # Job runs indefinitely (vLLM server never completes)
  # We don't set completions - the job just keeps running
  parallelism: 1
  backoffLimit: 3  # Retry up to 3 times if it crashes

  # Keep the job around for debugging even after it "completes" (which it won't)
  ttlSecondsAfterFinished: 86400  # 24 hours

  template:
    metadata:
      labels:
        app: vllm
        model: {{ model_key }}
        project: grace-experiments
        component: model-server
    spec:
      restartPolicy: OnFailure  # Restart the pod if vLLM crashes

      # Node selector for specific GPU type
      nodeSelector:
        nvidia.com/gpu.product: {{ gpu.node_selector }}

      # Tolerations for GPU nodes
      tolerations:
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule

      containers:
      - name: vllm-server
        image: vllm/vllm-openai:latest
        imagePullPolicy: Always

        command:
          - python3
          - -m
          - vllm.entrypoints.openai.api_server

        args:
{{ vllm_args | indent(10) }}

        ports:
          - containerPort: 8000
            name: http
            protocol: TCP

        resources:
          requests:
            nvidia.com/gpu: {{ gpu.count }}
            memory: {{ resources.memory_request }}
            cpu: "{{ resources.cpu_request }}"
          limits:
            nvidia.com/gpu: {{ gpu.count }}
            memory: {{ resources.memory_limit }}
            cpu: "{{ resources.cpu_limit }}"

        # Health probes - vLLM takes a while to start
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 600  # 10 min for model loading
          periodSeconds: 30
          timeoutSeconds: 10
          failureThreshold: 3

        readinessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 600  # 10 min for model loading
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3

        env:
          - name: HF_HOME
            value: /models/.cache
          - name: TRANSFORMERS_CACHE
            value: /models/.cache
          - name: HF_TOKEN
            valueFrom:
              secretKeyRef:
                name: hf-token-thomas
                key: HF_TOKEN
          - name: TRANSFORMERS_VERBOSITY
            value: info
          - name: HF_HUB_DISABLE_PROGRESS_BARS
            value: "0"
          - name: HF_HUB_ENABLE_HF_TRANSFER
            value: "1"

        volumeMounts:
          - name: shm
            mountPath: /dev/shm
          - name: model-cache
            mountPath: /models

      volumes:
        - name: shm
          emptyDir:
            medium: Memory
            sizeLimit: {{ resources.shm_size }}
        - name: model-cache
          persistentVolumeClaim:
            claimName: {{ pvcs.model_cache }}
